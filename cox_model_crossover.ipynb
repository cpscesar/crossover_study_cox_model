{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OM-pOSOUKDPl"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd  # For data manipulation and analysis\n",
        "import numpy as np  # For numerical operations\n",
        "from itertools import cycle  # For creating iterators that cycle through a sequence\n",
        "from lifelines import CoxTimeVaryingFitter  # For fitting time-varying Cox proportional hazards models\n",
        "from sklearn.preprocessing import StandardScaler  # For standardizing data features\n",
        "from scipy.stats import shapiro  # For conducting Shapiro-Wilk tests for normality of data\n",
        "import glob  # For finding all files matching a specified pattern\n",
        "import logging  # For logging information and debugging\n",
        "import sys  # For system-specific parameters and functions\n",
        "import seaborn as sns  # For data visualization, especially for statistical graphics\n",
        "import matplotlib.pyplot as plt  # For creating plots and figures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xbKvvcS8KDMz"
      },
      "outputs": [],
      "source": [
        "# Define the number of years, weeks per year, and number of health regions\n",
        "num_years = 8\n",
        "weeks_per_year = 52\n",
        "num_regions = 7\n",
        "\n",
        "# Create a date range for 8 years (from 2015 to 2022), with weekly frequency\n",
        "dates = pd.date_range(start='2015-01-01', periods=num_years * weeks_per_year, freq='W-MON')\n",
        "\n",
        "# Generate sample data for each health region and each week\n",
        "data = {\n",
        "    'DateWeek': np.tile(dates, num_regions),  # Repeating date range for each region\n",
        "    'Year': np.tile(dates.year, num_regions),  # Repeating years for each region\n",
        "    'WeekNum': np.tile(range(1, weeks_per_year + 1), num_years * num_regions),  # Weekly numbers for each year and region\n",
        "    'HealthReg': np.repeat(range(1, num_regions + 1), num_years * weeks_per_year),  # Health region identifiers\n",
        "    'HealthRegPopulation': np.repeat([np.random.randint(300000, 500000) for _ in range(num_regions)], num_years * weeks_per_year),  # Population of each health region\n",
        "    'MalePopulation': np.repeat([np.random.randint(200000, 300000) for _ in range(num_regions)], num_years * weeks_per_year),  # Male population in each region\n",
        "    'FemalePopulation': np.repeat([np.random.randint(100000, 200000) for _ in range(num_regions)], num_years * weeks_per_year),  # Female population in each region\n",
        "    'Sex': np.random.choice([0, 1], num_years * weeks_per_year * num_regions),  # Randomly assigning sex (0 for female, 1 for male)\n",
        "    'AdmissType': np.random.choice([1, 2, 3], num_years * weeks_per_year * num_regions),  # Randomly assigning admission types\n",
        "    'Hospitalisations': np.random.randint(0, 100, num_years * weeks_per_year * num_regions),  # Number of hospitalizations\n",
        "    'Event': np.random.choice([0, 1], num_years * weeks_per_year * num_regions),  # Binary event indicator (e.g., an occurrence of a certain event)\n",
        "    'WeeklyMaxAQHI': np.random.uniform(0, 10, num_years * weeks_per_year * num_regions),  # Weekly maximum Air Quality Health Index (AQHI)\n",
        "    'Baseline': np.random.choice([0, 1], num_years * weeks_per_year * num_regions)  # Binary baseline indicator\n",
        "}\n",
        "\n",
        "# Create DataFrame from the generated data\n",
        "synthetic_df = pd.DataFrame(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fxg7XI_AKDGV"
      },
      "outputs": [],
      "source": [
        "# Initialize the rate columns with zeros\n",
        "synthetic_df[\"MaleHospitalizationRate\"] = 0.0\n",
        "synthetic_df[\"FemaleHospitalizationRate\"] = 0.0\n",
        "\n",
        "# Calculate rates of hospitalization per 100 inhabitants for each sex\n",
        "synthetic_df.loc[synthetic_df[\"Sex\"] == 0, \"MaleHospitalizationRate\"] = (synthetic_df[\"Hospitalisations\"] / synthetic_df[\"MalePopulation\"]) * 100\n",
        "synthetic_df.loc[synthetic_df[\"Sex\"] == 1, \"FemaleHospitalizationRate\"] = (synthetic_df[\"Hospitalisations\"] / synthetic_df[\"FemalePopulation\"]) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tgkNT7cVKDCs"
      },
      "outputs": [],
      "source": [
        "# Sort the dataframe by HealthReg and DateWeek to ensure proper ordering\n",
        "synthetic_df = synthetic_df.sort_values(['HealthReg', 'DateWeek'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define lists for health regions, admission types, and sex\n",
        "reg = [1, 2, 3, 4, 5, 6, 7]\n",
        "admission = [1, 2, 3, 4, 5]\n",
        "sex = [0, 1]\n",
        "\n",
        "# Loop through each combination of health region, admission type, and sex\n",
        "for x in reg:\n",
        "    for y in admission:\n",
        "        for z in sex:\n",
        "            # Filter the DataFrame for the current region, admission type, and sex\n",
        "            group = synthetic_df[(synthetic_df['HealthReg'] == x) & \n",
        "                                 (synthetic_df['AdmissType'] == y) & \n",
        "                                 (synthetic_df['Sex'] == z)]\n",
        "\n",
        "            # Choose the correct column based on sex\n",
        "            if z == 1:\n",
        "                health_admission_col = 'FemaleHospitalizationRate'\n",
        "            else:\n",
        "                health_admission_col = 'MaleHospitalizationRate'\n",
        "\n",
        "            # Calculate the threshold for the top 10% of health admissions\n",
        "            threshold = group[health_admission_col].quantile(0.9)\n",
        "            print(f'Region: {x}, Admission Type: {y}, Sex: {z}, Threshold: {threshold}')\n",
        "\n",
        "            # Assign Baseline2 and Event2 based on the top 10% of health admissions\n",
        "            group['Baseline2'] = group[health_admission_col].apply(lambda val: 1 if val >= threshold else 0)\n",
        "            group['Event2'] = group['Baseline2']  # Event2 is the same as Baseline2\n",
        "\n",
        "            # Update the original DataFrame with the new columns\n",
        "            synthetic_df.loc[group.index, 'Baseline2'] = group['Baseline2']\n",
        "            synthetic_df.loc[group.index, 'Event2'] = group['Event2']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert 'DateWeek' column to datetime type if it's not already\n",
        "synthetic_df['DateWeek'] = pd.to_datetime(synthetic_df['DateWeek'])\n",
        "\n",
        "# Function to check for lag between AQHI event week and increase in health admissions\n",
        "def check_lag(df, health_admission_col):\n",
        "    lags = []\n",
        "    event_weeks = df[df['Event2'] == 1]['DateWeek']  # Identify weeks with an AQHI event\n",
        "\n",
        "    for event_week in event_weeks:\n",
        "        # Get the weeks following the AQHI event week\n",
        "        following_weeks = df[df['DateWeek'] > event_week]\n",
        "        \n",
        "        # Find the first week with an increase in health admissions after the event week\n",
        "        for i, row in following_weeks.iterrows():\n",
        "            if row[health_admission_col] >= df[health_admission_col].quantile(0.9):\n",
        "                lag = (row['DateWeek'] - event_week).days // 7  # Calculate lag in weeks\n",
        "                lags.append(lag)\n",
        "                break\n",
        "\n",
        "    return lags\n",
        "\n",
        "# Initialize dictionary to store lags for different groups\n",
        "lags_dict = {'HealthReg': [], 'AdmissType': [], 'Sex': [], 'LagInWeeks': []}\n",
        "\n",
        "# Iterate over regions, admission types, and sexes\n",
        "for x in reg:\n",
        "    for y in admission:\n",
        "        for z in sex:\n",
        "            # Filter the dataset for the specific group\n",
        "            group = synthetic_df[(synthetic_df['HealthReg'] == x) & \n",
        "                                 (synthetic_df['AdmissType'] == y) & \n",
        "                                 (synthetic_df['Sex'] == z)]\n",
        "\n",
        "            # Choose the correct column for health admissions based on sex\n",
        "            if z == 1:\n",
        "                health_admission_col = 'FemaleHospitalizationRate'\n",
        "            else:\n",
        "                health_admission_col = 'MaleHospitalizationRate'\n",
        "\n",
        "            # Check for lag in the group\n",
        "            lags = check_lag(group, health_admission_col)\n",
        "\n",
        "            # Store results in the dictionary\n",
        "            for lag in lags:\n",
        "                lags_dict['HealthReg'].append(x)\n",
        "                lags_dict['AdmissType'].append(y)\n",
        "                lags_dict['Sex'].append(z)\n",
        "                lags_dict['LagInWeeks'].append(lag)\n",
        "\n",
        "# Convert the dictionary to a DataFrame for easier analysis\n",
        "lags_df = pd.DataFrame(lags_dict)\n",
        "\n",
        "# Plot histogram of lag in weeks\n",
        "lags_df.hist('LagInWeeks')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a function to check for lag between AQHI event week and an increase in health admissions\n",
        "def check_lag(df, airindex_col):\n",
        "    lags = []\n",
        "    event_weeks = df[df['Event2'] == 1]['DateWeek']\n",
        "\n",
        "    # Iterate over each event week where an event occurred\n",
        "    for event_week in event_weeks:\n",
        "        # Get the weeks prior to the AQHI event week, sorted in descending order\n",
        "        previous_weeks = df[df['DateWeek'] < event_week].sort_values(by='DateWeek', ascending=False)\n",
        "\n",
        "        # Find the first week with AQHI >= 7 before the event week\n",
        "        for i, row in previous_weeks.iterrows():\n",
        "            if row[airindex_col] >= 7:\n",
        "                # Calculate the lag in weeks between the event and the AQHI week\n",
        "                lag = (event_week - row['DateWeek']).days // 7\n",
        "                lags.append({\n",
        "                    'EventWeek': event_week,\n",
        "                    'AQHIWeek': row['DateWeek'],\n",
        "                    'LagInWeeks': lag\n",
        "                })\n",
        "                break  # Stop looking once the first lag is found\n",
        "\n",
        "    return lags\n",
        "\n",
        "# Initialize a dictionary to store lag data for different groups\n",
        "lags_dict = {'HealthReg': [], 'AdmissType': [], 'Sex': [], 'EventWeek': [], 'AQHIWeek': [], 'LagInWeeks': []}\n",
        "\n",
        "# Iterate over each combination of health region, admission type, and sex\n",
        "for x in reg:\n",
        "    for y in admission:\n",
        "        for z in sex:\n",
        "            group = synthetic_df[(synthetic_df['HealthReg'] == x) &\n",
        "                                 (synthetic_df['AdmissType'] == y) &\n",
        "                                 (synthetic_df['Sex'] == z)]\n",
        "\n",
        "            # The column representing AQHI levels\n",
        "            airindex_col = 'WeeklyMaxAQHI'\n",
        "\n",
        "            # Check for lags in the current group\n",
        "            lags = check_lag(group, airindex_col)\n",
        "\n",
        "            # Append the results to the dictionary\n",
        "            for lag in lags:\n",
        "                lags_dict['HealthReg'].append(x)\n",
        "                lags_dict['AdmissType'].append(y)\n",
        "                lags_dict['Sex'].append(z)\n",
        "                lags_dict['EventWeek'].append(lag['EventWeek'])\n",
        "                lags_dict['AQHIWeek'].append(lag['AQHIWeek'])\n",
        "                lags_dict['LagInWeeks'].append(lag['LagInWeeks'])\n",
        "\n",
        "# Convert the dictionary into a DataFrame for easier analysis\n",
        "lags_df = pd.DataFrame(lags_dict)\n",
        "\n",
        "# Add a column to indicate whether the lag is within 5 weeks\n",
        "lags_df['LagCondition'] = lags_df['LagInWeeks'].apply(lambda x: 1 if x <= 5 else 0)\n",
        "\n",
        "# Merge the lag information back into the original dataset\n",
        "synthetic_df = pd.merge(\n",
        "    synthetic_df,\n",
        "    lags_df[['HealthReg', 'AdmissType', 'Sex', 'EventWeek', 'LagCondition']],\n",
        "    how='left',\n",
        "    left_on=['HealthReg', 'AdmissType', 'Sex', 'DateWeek'],\n",
        "    right_on=['HealthReg', 'AdmissType', 'Sex', 'EventWeek']\n",
        ")\n",
        "\n",
        "# Fill NaN values with 0 for rows where the condition is not met\n",
        "synthetic_df['LagCondition'] = synthetic_df['LagCondition'].fillna(0).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating new columns 'Baseline3' and 'Event3' by copying the values from the 'LagCondition' column\n",
        "synthetic_df['Baseline3'] = synthetic_df['LagCondition']\n",
        "synthetic_df['Event3'] = synthetic_df['LagCondition']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QB1pJtZmov4K",
        "outputId": "8a3160fa-8b61-4e8b-b8a8-40b3992b0049"
      },
      "outputs": [],
      "source": [
        "# Sample regions, admission types, and sex categories\n",
        "reg = [1, 2, 3, 4, 5, 6, 7]\n",
        "admission = [1, 2, 3, 4, 5]\n",
        "sex = [0, 1]\n",
        "\n",
        "# Loop through each region, admission type, and sex category\n",
        "for x in reg:\n",
        "    for y in admission:\n",
        "        for z in sex:\n",
        "            # Filter the dataframe based on the current region, admission type, and sex\n",
        "            group = synthetic_df[(synthetic_df['HealthReg'] == x) & \n",
        "                                 (synthetic_df['AdmissType'] == y) & \n",
        "                                 (synthetic_df['Sex'] == z)]\n",
        "            \n",
        "            # Separate the group into baselines and controls\n",
        "            baselines = group[group['Baseline3'] == 1.0]\n",
        "            controls = group[group['Baseline3'] == 0.0]\n",
        "\n",
        "            print(f\"\\nProcessing HealthReg: {group['HealthReg'].iloc[0]}\")\n",
        "            print(f\"Processing AdmissType: {group['AdmissType'].iloc[0]}\")\n",
        "            print(f\"Processing Sex: {group['Sex'].iloc[0]}\")\n",
        "            print(f\"Baselines in this HealthReg: {len(baselines)}\")\n",
        "            print(f\"Potential controls in this HealthReg: {len(controls)}\")\n",
        "\n",
        "            # Group baselines into batches of 1 consecutive weeks\n",
        "            baselines['BatchID'] = (baselines['WeekNum'] != baselines['WeekNum'].shift() + 1).cumsum()\n",
        "            baseline_batches = baselines.groupby('BatchID')\n",
        "\n",
        "            control_cases = []\n",
        "            matching_id = 1\n",
        "\n",
        "            # Process each batch of baselines\n",
        "            for batch_id, batch in baseline_batches:\n",
        "                print(f\"\\nProcessing Batch ID: {batch_id}\")\n",
        "                print(f\"Batch size: {len(batch)}\")\n",
        "\n",
        "                if len(batch) < 4:\n",
        "                    print(\"Skipping incomplete batch\")\n",
        "                    continue  # Skip incomplete batches\n",
        "\n",
        "                # Extract baseline year and weeks for the batch\n",
        "                baseline_year = batch['Year'].iloc[0]\n",
        "                baseline_weeks = batch['WeekNum'].tolist()\n",
        "                print(f\"Baseline Year: {baseline_year}\")\n",
        "                print(f\"Baseline Weeks: {baseline_weeks}\")\n",
        "\n",
        "                # Identify potential control years excluding the baseline year\n",
        "                control_years = controls['Year'].unique()\n",
        "                control_years = control_years[control_years != baseline_year]\n",
        "                print(f\"Potential Control Years: {control_years}\")\n",
        "\n",
        "                if len(control_years) < 7:\n",
        "                    print(\"Not enough control years available. Skipping batch.\")\n",
        "                    continue  # Not enough control years available\n",
        "\n",
        "                # Cycle through control years and weeks\n",
        "                week_cycle = cycle(baseline_weeks)\n",
        "\n",
        "                for year in control_years:\n",
        "                    for _ in baseline_weeks:\n",
        "                        week = next(week_cycle)\n",
        "                        control_case = controls[(controls['Year'] == year) & (controls['WeekNum'] == week)]\n",
        "                        print('Control year', year)\n",
        "                        print('Control week', week)\n",
        "\n",
        "                        if not control_case.empty:\n",
        "                            control_case = control_case.iloc[0].copy()\n",
        "                            control_case['MatchingID'] = matching_id\n",
        "                            control_cases.append(control_case)\n",
        "                            print(f\"Matched: Baseline Year {baseline_year}, Week {week} with Control Year {year}, MatchID {matching_id}\")\n",
        "                        else:\n",
        "                            print(f\"No match found for Baseline Year {baseline_year}, Week {week}\")\n",
        "\n",
        "                # Assign MatchingID to baseline batch\n",
        "                batch['MatchingID'] = matching_id\n",
        "                control_cases.extend(batch.to_dict('records'))\n",
        "\n",
        "                print(f\"Assigned MatchingID {matching_id} to batch\")\n",
        "\n",
        "                matching_id += 1\n",
        "\n",
        "                # Convert all elements to dictionaries\n",
        "                data_dicts = [item.to_dict() if isinstance(item, pd.Series) else item for item in control_cases]\n",
        "\n",
        "                # Create DataFrame from list of dictionaries\n",
        "                result_df = pd.DataFrame(data_dicts)\n",
        "\n",
        "                print(f\"Region: {x}, Admission: {y}, Sex {z}, Total rows: {len(result_df)}\")\n",
        "                print(f\"Baseline cases: {result_df['Baseline3'].sum()}\")\n",
        "                print(f\"Control cases: {len(result_df) - result_df['Baseline2'].sum()}\")\n",
        "\n",
        "                # Save the result to an Excel file\n",
        "                result_df.to_excel(f'ControlCases_Reg_{x}_Adm_{y}_Sex_{z}.xlsx', index=False)\n",
        "                print(f'Saved: ControlCases_Reg_{x}_Adm_{y}_Sex_{z}.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_control(file_path):\n",
        "    \"\"\"\n",
        "    Read and filter data from an Excel file.\n",
        "    \n",
        "    Parameters:\n",
        "    file_path (str): The path to the Excel file.\n",
        "    \n",
        "    Returns:\n",
        "    pd.DataFrame: The filtered DataFrame.\n",
        "    \"\"\"\n",
        "    df_filtered = pd.read_excel(file_path)\n",
        "    return df_filtered\n",
        "\n",
        "# Path to the directory containing the saved Excel files\n",
        "file_paths = glob.glob('ControlCases_Reg_*.xlsx')\n",
        "\n",
        "# Create an Excel writer to save the results in a single file\n",
        "with pd.ExcelWriter('filtered_datasets.xlsx', engine='xlsxwriter') as writer:\n",
        "    # Process each file in the directory\n",
        "    for file_path in file_paths:\n",
        "        # Extract the sheet name from the file name by removing directory path and file extension\n",
        "        sheet_name = file_path.split('/')[-1].replace('ControlCases_', '').replace('.xlsx', '')\n",
        "        \n",
        "        # Filter the data using the filter_control function\n",
        "        df_filtered = filter_control(file_path)\n",
        "        \n",
        "        # Save the filtered DataFrame to the Excel file with the extracted sheet name\n",
        "        df_filtered.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "\n",
        "        print(f'Saved sheet {sheet_name}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the function to perform correlation and normality tests on a given file\n",
        "def analyze_file(file_path):\n",
        "    df = pd.read_excel(file_path)  # Read data from the Excel file\n",
        "    df.fillna(0, inplace=True)  # Replace NaN values with 0 for consistency\n",
        "    df['WeeklyMaxAQHI'] += 1e-6  # Small adjustment to avoid log(0)\n",
        "    df['WeeklyMaxAQHI'] = np.log(df['WeeklyMaxAQHI'])  # Apply log transformation for normalization\n",
        "    \n",
        "    # Calculate the correlation matrix for the specified columns\n",
        "    correlation_matrix = df[[\"WeeklyMaxAQHI\", \"Baseline3\"]].corr()\n",
        "    \n",
        "    # Perform Shapiro-Wilk test for normality on the 'WeeklyMaxAQHI' column\n",
        "    normality_results = df[[\"WeeklyMaxAQHI\"]].apply(lambda x: shapiro(x)[1])\n",
        "    normality_results = normality_results.to_frame(name='p_value')  # Convert results to DataFrame\n",
        "    \n",
        "    return correlation_matrix, normality_results\n",
        "\n",
        "# Path to the directory containing the saved Excel files\n",
        "file_paths = glob.glob('ControlCases_Reg_*.xlsx')\n",
        "\n",
        "# Create an Excel writer to save the results in a new file\n",
        "writer = pd.ExcelWriter('Correlation_Results_With_Normality.xlsx', engine='xlsxwriter')\n",
        "\n",
        "# Process each Excel file in the specified directory\n",
        "for file_path in file_paths:\n",
        "    # Extract sheet name from the file name\n",
        "    sheet_name = file_path.split('/')[-1].replace('ControlCases_', '').replace('.xlsx', '')\n",
        "    \n",
        "    # Analyze the file to obtain correlation and normality test results\n",
        "    correlation_matrix, normality_results = analyze_file(file_path)\n",
        "    \n",
        "    # Save the correlation matrix and normality results to the Excel writer\n",
        "    correlation_matrix.to_excel(writer, sheet_name=sheet_name)\n",
        "    normality_results.to_excel(writer, sheet_name=sheet_name, startrow=correlation_matrix.shape[0] + 2)\n",
        "\n",
        "    print(f'Saved sheet {file_path}')\n",
        "\n",
        "# Finalize and save the Excel file with all results\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the function to perform the grouping and aggregation\n",
        "def analyze_sheet(df):\n",
        "    # Group by 'MatchingID', 'Baseline3', and 'Year' and calculate the mean of 'MalePopulation' and 'FemalePopulation'\n",
        "    mean_population = df.groupby(['MatchingID', 'Baseline3', 'Year'])[['MalePopulation', 'FemalePopulation']].mean()\n",
        "\n",
        "    # Group by 'MatchingID', 'Baseline3', and 'Year' and calculate the sum of 'Hospitalisations'\n",
        "    sum_hospitalization = df.groupby(['MatchingID', 'Baseline3', 'Year'])['Hospitalisations'].sum()\n",
        "\n",
        "    # Group by 'MatchingID', 'Baseline3', and 'Year' and calculate the max of 'WeeklyMaxAQHI'\n",
        "    max_weekly_max_aqhi = df.groupby(['MatchingID', 'Baseline3', 'Year'])['WeeklyMaxAQHI'].max()\n",
        "\n",
        "    # Group by 'MatchingID', 'Baseline3', and 'Year' and calculate the max of 'Event3'\n",
        "    max_event = df.groupby(['MatchingID', 'Baseline3', 'Year'])['Event3'].max()\n",
        "\n",
        "    # Define a fixed duration variable (e.g., 35 days)\n",
        "    duration = 35\n",
        "\n",
        "    # Combine the results into a single DataFrame\n",
        "    result = pd.DataFrame({\n",
        "        'MeanMalePopulation': mean_population['MalePopulation'],\n",
        "        'MeanFemalePopulation': mean_population['FemalePopulation'],\n",
        "        'SumHospitalization': sum_hospitalization,\n",
        "        'MaxWeeklyMaxAQHI': max_weekly_max_aqhi,\n",
        "        'MaxEvent': max_event,\n",
        "        'Duration': duration\n",
        "    }).reset_index()\n",
        "\n",
        "    # Calculate hospitalization rate per 100 individuals in the female population\n",
        "    result['HospRate'] = result['SumHospitalization'] / result['MeanFemalePopulation'] * 100\n",
        "\n",
        "    return result\n",
        "\n",
        "# Read the filtered datasets from an Excel file with multiple sheets\n",
        "file_path = 'filtered_datasets.xlsx'\n",
        "excel_file = pd.ExcelFile(file_path)\n",
        "\n",
        "# Create an Excel writer to save the aggregated results\n",
        "with pd.ExcelWriter('Aggregated_Results.xlsx', engine='xlsxwriter') as writer:\n",
        "    # Process each sheet in the Excel file\n",
        "    for sheet_name in excel_file.sheet_names:\n",
        "        # Read the sheet into a DataFrame\n",
        "        df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
        "        \n",
        "        # Analyze the DataFrame using the defined function\n",
        "        result = analyze_sheet(df)\n",
        "        \n",
        "        # Save the results to the new Excel file\n",
        "        result.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "\n",
        "        print(f'Saved sheet {sheet_name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the function to perform data preprocessing and Cox model fitting\n",
        "def analyze_sheet(df):\n",
        "    # Fill missing values with 0 and perform necessary transformations\n",
        "    df.fillna(0, inplace=True)\n",
        "    df['MaxWeeklyMaxAQHI'] += 1e-6  # Avoid log(0) by adding a small constant\n",
        "    df['MaxWeeklyMaxAQHI'] = np.log(df['MaxWeeklyMaxAQHI'])  # Log transform for AQHI\n",
        "    df['HospRate'] += 1e-6  # Avoid issues with zero rates\n",
        "    df['AQHI_Baseline_Interaction'] = df['MaxWeeklyMaxAQHI'] * df['Baseline3']  # Interaction term\n",
        "    \n",
        "    # Select relevant columns for the analysis\n",
        "    selected_weeks_df = df[['MaxWeeklyMaxAQHI', \"Baseline3\", 'Duration', 'MaxEvent', 'AQHI_Baseline_Interaction']]\n",
        "\n",
        "    # Fit the Cox Proportional Hazards model\n",
        "    cph = CoxPHFitter(penalizer=0.1)  # Initialize model with a penalizer to handle multicollinearity\n",
        "    cph.fit(selected_weeks_df, duration_col='Duration', event_col='MaxEvent', show_progress=True)  # Fit the model\n",
        "\n",
        "    # Extract the summary of the model fit\n",
        "    summary = cph.summary\n",
        "    \n",
        "    return summary\n",
        "\n",
        "# Define the file path for the Excel file containing filtered datasets\n",
        "file_path = 'Aggregated_Results.xlsx'\n",
        "excel_file = pd.ExcelFile(file_path)  # Load the Excel file\n",
        "\n",
        "# Create an Excel writer object to save the model results\n",
        "with pd.ExcelWriter('CoxPH_Results.xlsx', engine='xlsxwriter') as writer:\n",
        "    # Iterate through each sheet in the Excel file\n",
        "    for sheet_name in excel_file.sheet_names:\n",
        "        # Load the data from the current sheet into a DataFrame\n",
        "        df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
        "        \n",
        "        # Apply the analysis function to the DataFrame\n",
        "        summary = analyze_sheet(df)\n",
        "        \n",
        "        # Save the analysis results to a new Excel file\n",
        "        summary.to_excel(writer, sheet_name=sheet_name, index=True)\n",
        "\n",
        "        print(f'Saved sheet {sheet_name}')  # Confirmation message"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
